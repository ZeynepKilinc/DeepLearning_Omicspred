# -*- coding: utf-8 -*-
"""Deep_LEARN_OMICSPRED.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YetOg2R0syK7ocOuDjZJ5Gi6ZnRRA-1_
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install umap-learn

import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import torch
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

import ast

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

data = pd.read_csv('/content/drive/My Drive/Deep Learning/updated_data_split.csv')

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt


data['C_str'] = data['C'].apply(lambda x: ' '.join(eval(x)) if isinstance(x, str) else '')
data['F_str'] = data['F'].apply(lambda x: ' '.join(eval(x)) if isinstance(x, str) else '')
data['P_str'] = data['P'].apply(lambda x: ' '.join(eval(x)) if isinstance(x, str) else '')
data['react_str'] = data['react'].apply(lambda x: ' '.join(eval(x)) if isinstance(x, str) else '')

data['combined'] = data[['C_str', 'F_str',  'react_str']].apply(lambda x: ' '.join(x), axis=1)
vectorizer = TfidfVectorizer(max_features=50)
X = vectorizer.fit_transform(data['combined']).toarray()
pca = PCA(n_components=3)
X_pca = pca.fit_transform(X)
kmeans = KMeans(n_clusters=5, random_state=42)
clusters = kmeans.fit_predict(X_pca)
data['Cluster'] = clusters
plt.figure(figsize=(10, 8))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='viridis', s=50)
plt.colorbar(label='Cluster')
plt.title('Gene Clustering Visualization')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.show()

explained_variance = pca.explained_variance_ratio_.sum()
print(f"Explained Variance by First Two Components: {explained_variance * 100:.2f}%")

from sklearn.metrics import silhouette_score
score = silhouette_score(X_pca, clusters)
print(f"Silhouette Score: {score}")

from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

sil_scores = []
cluster_range = range(2, 15)
for k in cluster_range:
    kmeans = KMeans(n_clusters=k, random_state=42).fit(X_pca)
    sil_scores.append(silhouette_score(X_pca, kmeans.labels_))

plt.plot(cluster_range, sil_scores, marker='o')
plt.xlabel('# Clusters')
plt.ylabel('Silhouette Score')
plt.title('Optimality Analysis')
plt.show()

data=df

import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

data = pd.read_csv('/content/drive/My Drive/Deep Learning/enzyme_table.csv',header=None)
column_names =  ['enzyme', 'substrate', 'product','reaction','class','gene']
data.columns = column_names



label_encoders = {}
for col in ['enzyme', 'reaction', 'class', 'gene']:
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col])
    label_encoders[col] = le

max_substrate_len = 5
max_product_len = 5

def pad_or_truncate(lst, max_len):
    return (lst[:max_len] + [0] * max(0, max_len - len(lst)))

data['substrate'] = data['substrate'].apply(lambda x: pad_or_truncate(
    [int(i) for i in x.strip('[]').split(',')] if isinstance(x, str) else [], max_substrate_len))
data['product'] = data['product'].apply(lambda x: pad_or_truncate(
    [int(i) for i in x.strip('[]').split(',')] if isinstance(x, str) else [], max_product_len))

X = data[['enzyme', 'reaction', 'class', 'substrate', 'product']].values
y = data['gene'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

class MetaboliteGeneDataset(Dataset):
    def __init__(self, X, y):
        self.enzyme = torch.tensor(X[:, 0], dtype=torch.long)
        self.reaction = torch.tensor(X[:, 1], dtype=torch.long)
        self.cls = torch.tensor(X[:, 2], dtype=torch.long)
        self.substrate = torch.tensor(np.stack(X[:, 3]), dtype=torch.long)
        self.product = torch.tensor(np.stack(X[:, 4]), dtype=torch.long)
        self.y = torch.tensor(y, dtype=torch.long)

    def __len__(self):
        return len(self.y)

    def __getitem__(self, idx):
        return {
            'enzyme': self.enzyme[idx],
            'reaction': self.reaction[idx],
            'cls': self.cls[idx],
            'substrate': self.substrate[idx],
            'product': self.product[idx],
            'label': self.y[idx]
        }

train_dataset = MetaboliteGeneDataset(X_train, y_train)
test_dataset = MetaboliteGeneDataset(X_test, y_test)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

class MetaboliteGeneModel(nn.Module):
    def __init__(self, vocab_size, embed_dim, num_classes, max_len):
        super(MetaboliteGeneModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=4),
            num_layers=2)
        self.fc = nn.Linear(embed_dim, num_classes)

    def forward(self, enzyme, reaction, cls, substrate, product):
        enzyme_emb = self.embedding(enzyme)
        reaction_emb = self.embedding(reaction)
        cls_emb = self.embedding(cls)
        substrate_emb = self.embedding(substrate).mean(dim=1)  # Average pooling
        product_emb = self.embedding(product).mean(dim=1)
        combined = torch.cat([enzyme_emb, reaction_emb, cls_emb, substrate_emb, product_emb], dim=1)
        transformer_output = self.transformer(combined.unsqueeze(0)).squeeze(0)
        out = self.fc(transformer_output)
        return out
vocab_size = max(data[['enzyme', 'reaction', 'class']].max()) + 1
embed_dim = 64
num_classes = len(label_encoders['gene'].classes_)
max_len = max_substrate_len + max_product_len
model = MetaboliteGeneModel(vocab_size, embed_dim, num_classes, max_len)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=10):
    train_losses = []
    test_accuracies = []

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0

        for batch in train_loader:
            optimizer.zero_grad()
            outputs = model(batch['enzyme'], batch['reaction'], batch['cls'], batch['substrate'], batch['product'])
            loss = criterion(outputs, batch['label'])
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        train_losses.append(running_loss / len(train_loader))
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for batch in test_loader:
                outputs = model(batch['enzyme'], batch['reaction'], batch['cls'], batch['substrate'], batch['product'])
                _, predicted = torch.max(outputs, 1)
                total += batch['label'].size(0)
                correct += (predicted == batch['label']).sum().item()

        test_accuracy = correct / total
        test_accuracies.append(test_accuracy)

        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}, Test Accuracy: {test_accuracy:.4f}")

    return train_losses, test_accuracies

train_losses, test_accuracies = train_model(model, train_loader, test_loader, criterion, optimizer)

column_names =  ['enzyme', 'substrate', 'product','reaction','class','gene']
df = pd.read_csv('/content/drive/My Drive/Deep Learning/enzyme_table.csv',header=None)
df.columns = column_names
df['substrate'] = df['substrate'].apply(ast.literal_eval)
df['product'] = df['product'].apply(ast.literal_eval)

new_rows = []

for _, row in df.iterrows():
    enzyme = row['enzyme']
    reaction = row['reaction']
    gene = row['gene']

    for compound in row['substrate']:
        new_rows.append({
            'compound': compound,
            'enzyme': enzyme,
            'role': 0,  # 0 for substrate
            'reaction': reaction,
            'gene': gene
        })

    for compound in row['product']:
        new_rows.append({
            'compound': compound,
            'enzyme': enzyme,
            'role': 1,  # 1 for product
            'reaction': reaction,
            'gene': gene
        })

new_df = pd.DataFrame(new_rows)
df=new_df
df = pd.DataFrame(df)

le_compound = LabelEncoder()
le_enzyme = LabelEncoder()
le_reaction = LabelEncoder()
le_gene = LabelEncoder()

df['compound_encoded'] = le_compound.fit_transform(df['compound'])
df['enzyme_encoded'] = le_enzyme.fit_transform(df['enzyme'])
df['reaction_encoded'] = le_reaction.fit_transform(df['reaction'])
df['gene_encoded'] = le_gene.fit_transform(df['gene'])

X = df[['compound_encoded', 'enzyme_encoded', 'role', 'reaction_encoded']].values
y = df['gene_encoded'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
class GeneDataset(Dataset):
    def __init__(self, features, labels):
        self.features = features
        self.labels = labels

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return {
            'features': torch.tensor(self.features[idx], dtype=torch.float32),
            'labels': torch.tensor(self.labels[idx], dtype=torch.long)
        }

train_dataset = GeneDataset(X_train, y_train)
test_dataset = GeneDataset(X_test, y_test)
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)
class GeneTransformerModel(nn.Module):
    def __init__(self, input_dim, num_classes, embed_dim=512, num_heads=4, num_layers=2):
        super(GeneTransformerModel, self).__init__()
        self.embedding = nn.Embedding(input_dim, embed_dim)
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads), num_layers=num_layers
        )
        self.fc = nn.Linear(embed_dim, num_classes)

    def forward(self, x):
        x = self.embedding(x.long())
        x = self.transformer(x)
        x = x.mean(dim=1)
        x = self.fc(x)  # Classification layer
        return x
input_dim = max(df[['compound_encoded', 'enzyme_encoded', 'role','reaction_encoded']].max()) + 1
num_classes=len(df['gene_encoded'].unique())
model=GeneTransformerModel(input_dim=input_dim, num_classes=num_classes)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0005)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report

train_losses = []
test_accuracies = []
for epoch in range(20):
    model.train()
    train_loss = 0
    for batch in train_loader:
        features = batch['features']
        labels = batch['labels']

        optimizer.zero_grad()
        outputs = model(features)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()

    train_losses.append(train_loss / len(train_loader))

    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for batch in test_loader:
            features = batch['features']
            labels = batch['labels']
            outputs = model(features)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    test_accuracy = correct / total
    test_accuracies.append(test_accuracy)

    print(f"Epoch {epoch+1}, Loss: {train_loss/len(train_loader):.4f}, Test Accuracy: {test_accuracy:.4f}")
plt.figure(figsize=(8, 6))
plt.plot(train_losses, label='Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss Over Epochs')
plt.legend()
plt.show()
plt.figure(figsize=(8, 6))
plt.plot(test_accuracies, label='Test Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Test Accuracy Over Epochs')
plt.legend()
plt.show()


print(classification_report(all_labels, all_predictions, target_names=label_encoders['gene'].classes_))

df.columns = column_names
df['substrate'] = df['substrate'].apply(ast.literal_eval)
df['product'] = df['product'].apply(ast.literal_eval)

new_rows = []
for _, row in df.iterrows():
    enzyme, reaction, gene = row['enzyme'], row['reaction'], row['gene']
    for compound in row['substrate']:
        new_rows.append({'compound': compound, 'enzyme': enzyme, 'role': 0, 'reaction': reaction, 'gene': gene})
    for compound in row['product']:
        new_rows.append({'compound': compound, 'enzyme': enzyme, 'role': 1, 'reaction': reaction, 'gene': gene})

df = pd.DataFrame(new_rows)
le_compound = LabelEncoder()
le_enzyme = LabelEncoder()
le_reaction = LabelEncoder()
le_gene = LabelEncoder()

df['compound_encoded'] = le_compound.fit_transform(df['compound'])
df['enzyme_encoded'] = le_enzyme.fit_transform(df['enzyme'])
df['reaction_encoded'] = le_reaction.fit_transform(df['reaction'])
df['gene_encoded'] = le_gene.fit_transform(df['gene'])

X = df[['compound_encoded', 'enzyme_encoded', 'role', 'reaction_encoded']].values
y = df['gene_encoded'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

class GeneDataset(Dataset):
    def __init__(self, features, labels):
        self.features = torch.tensor(features, dtype=torch.float32)
        self.labels = torch.tensor(labels, dtype=torch.long)

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]
train_dataset = GeneDataset(X_train, y_train)
test_dataset = GeneDataset(X_test, y_test)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
class GeneMLPModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_classes):
        super(GeneMLPModel, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)
        self.fc3 = nn.Linear(hidden_dim // 2, num_classes)

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x

input_dim=X.shape[1]
hidden_dim=128
num_classes = len(df['gene_encoded'].unique())
model = GeneMLPModel(input_dim, hidden_dim, num_classes)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
num_epochs = 20
for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    for features, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(features)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
    print(f"Epoch {epoch+1}, Loss: {train_loss / len(train_loader):.4f}")
model.eval()
correct = 0
total = 0
with torch.no_grad():
    for features, labels in test_loader:
        outputs = model(features)
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

accuracy = 100 * correct / total
print(f"Test Accuracy: {accuracy:.2f}%")

import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
df=new_df
le_compound = LabelEncoder()
le_enzyme = LabelEncoder()
le_reaction = LabelEncoder()

df['compound_encoded'] = le_compound.fit_transform(df['compound'])
df['enzyme_encoded'] = le_enzyme.fit_transform(df['enzyme'])
df['reaction_encoded'] = le_reaction.fit_transform(df['reaction'])

features = df[['compound_encoded', 'enzyme_encoded', 'role', 'reaction_encoded']]
n_clusters = 8
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
df['gene_cluster'] = kmeans.fit_predict(features)

X = df[['compound_encoded', 'enzyme_encoded', 'role', 'reaction_encoded']]
y = df['gene_cluster']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

model = XGBClassifier(enable_categorical=True, use_label_encoder=False, eval_metric='mlogloss')
model.fit(X_train, y_train)

accuracy = model.score(X_test, y_test)
print(f"Model Accuracy: {accuracy}")
print(df[['gene', 'gene_cluster']])

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
pca = PCA(n_components=2)
reduced_features = pca.fit_transform(features)
plt.figure(figsize=(8, 6))
for cluster in range(n_clusters):
    cluster_points = reduced_features[df['gene_cluster'] == cluster]
    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {cluster}')

plt.title('K-Means Clusters Visualization')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.legend()
plt.show()

from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import LabelEncoder
le_compound = LabelEncoder()
le_enzyme = LabelEncoder()
le_reaction = LabelEncoder()

df['compound_encoded'] = le_compound.fit_transform(df['compound'])
df['enzyme_encoded'] = le_enzyme.fit_transform(df['enzyme'])
df['reaction_encoded'] = le_reaction.fit_transform(df['reaction'])
features = df[['compound_encoded', 'enzyme_encoded', 'role', 'reaction_encoded']]
tsne = TSNE(n_components=2, random_state=42, perplexity=30)
tsne_results = tsne.fit_transform(features)
df['tsne_1'] = tsne_results[:, 0]
df['tsne_2'] = tsne_results[:, 1]

plt.figure(figsize=(8, 6))
plt.scatter(df['tsne_1'], df['tsne_2'], alpha=0.7, s=100)
plt.title('t-SNE Visualization')
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')
plt.show()

import pandas as pd
import ast
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

df['substrate'] = df['substrate'].apply(ast.literal_eval)
df['product'] = df['product'].apply(ast.literal_eval)

new_rows = []
for _, row in df.iterrows():
    enzyme, reaction, gene = row['enzyme'], row['reaction'], row['gene']
    for compound in row['substrate']:
        new_rows.append({'compound': compound, 'enzyme': enzyme, 'role': 0, 'reaction': reaction, 'gene': gene})
    for compound in row['product']:
        new_rows.append({'compound': compound, 'enzyme': enzyme, 'role': 1, 'reaction': reaction, 'gene': gene})

df = pd.DataFrame(new_rows)

le_compound = LabelEncoder()
le_enzyme = LabelEncoder()
le_reaction = LabelEncoder()
le_gene = LabelEncoder()

df['compound_encoded'] = le_compound.fit_transform(df['compound'])
df['enzyme_encoded'] = le_enzyme.fit_transform(df['enzyme'])
df['reaction_encoded'] = le_reaction.fit_transform(df['reaction'])
df['gene_encoded'] = le_gene.fit_transform(df['gene'])

X = df[['compound_encoded', 'enzyme_encoded', 'role', 'reaction_encoded']].values
y = df['gene_encoded'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

class GeneDataset(Dataset):
    def __init__(self, features, labels):
        self.features = torch.tensor(features, dtype=torch.float32)
        self.labels = torch.tensor(labels, dtype=torch.long)

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

train_dataset = GeneDataset(X_train, y_train)
test_dataset = GeneDataset(X_test, y_test)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

class GeneMLPModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_classes):
        super(GeneMLPModel, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)
        self.fc3 = nn.Linear(hidden_dim // 2, num_classes)

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x

input_dim = X.shape[1]
hidden_dim = 128
num_classes = len(df['gene_encoded'].unique())
model = GeneMLPModel(input_dim, hidden_dim, num_classes)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

num_epochs = 20
for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    for features, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(features)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
    print(f"Epoch {epoch+1}, Loss: {train_loss / len(train_loader):.4f}")

model.eval()
correct = 0
total = 0
with torch.no_grad():
    for features, labels in test_loader:
        outputs = model(features)
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

accuracy = 100 * correct / total
print(f"Test Accuracy: {accuracy:.2f}%")

import umap
import matplotlib.pyplot as plt

features = df[['compound_encoded', 'enzyme_encoded', 'role', 'reaction_encoded']]

umap_reducer = umap.UMAP(n_components=2, random_state=42)
umap_results = umap_reducer.fit_transform(features)

df['umap_1'] = umap_results[:, 0]
df['umap_2'] = umap_results[:, 1]

plt.figure(figsize=(8, 6))
plt.scatter(df['umap_1'], df['umap_2'], c='green', alpha=0.7, s=100)
plt.title('UMAP Visualization')
plt.xlabel('UMAP Component 1')
plt.ylabel('UMAP Component 2')
plt.show()

print(df['gene'].value_counts())

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
new_df['compound_encoded'] = le.fit_transform(new_df['compound'])
new_df['enzyme_encoded'] = le.fit_transform(new_df['enzyme'])
new_df['reaction_encoded'] = le.fit_transform(new_df['reaction'])

new_df['gene_encoded'] = le.fit_transform(new_df['gene'])
X = new_df[['compound_encoded', 'enzyme_encoded', 'role', 'reaction_encoded']]
y = new_df['gene_encoded']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)

import pandas as pd
import ast
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

column_names = ['enzyme', 'substrate', 'product', 'reaction', 'class', 'gene']
df = pd.read_csv(file_path, header=None)
df.columns = column_names
df['substrate'] = df['substrate'].apply(ast.literal_eval)
df['product'] = df['product'].apply(ast.literal_eval)
new_rows = []
for _, row in df.iterrows():
    enzyme, reaction, gene = row['enzyme'], row['reaction'], row['gene']
    for compound in row['substrate']:
        new_rows.append({'compound': compound, 'enzyme': enzyme, 'role': 0, 'reaction': reaction, 'gene': gene})
    for compound in row['product']:
        new_rows.append({'compound': compound, 'enzyme': enzyme, 'role': 1, 'reaction': reaction, 'gene': gene})

df = pd.DataFrame(new_rows)

le_compound = LabelEncoder()
le_enzyme = LabelEncoder()
le_reaction = LabelEncoder()
le_gene = LabelEncoder()
df['compound_encoded'] = le_compound.fit_transform(df['compound'])
df['enzyme_encoded'] = le_enzyme.fit_transform(df['enzyme'])
df['reaction_encoded'] = le_reaction.fit_transform(df['reaction'])
df['gene_encoded'] = le_gene.fit_transform(df['gene'])
X = df[['compound_encoded', 'enzyme_encoded', 'role', 'reaction_encoded']].values
y = df['gene_encoded'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

class GeneDataset(Dataset):
    def __init__(self, features, labels):
        self.features = torch.tensor(features, dtype=torch.float32)
        self.labels = torch.tensor(labels, dtype=torch.long)

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

train_dataset = GeneDataset(X_train, y_train)
test_dataset = GeneDataset(X_test, y_test)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

class GeneMLPModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_classes):
        super(GeneMLPModel, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)
        self.fc3 = nn.Linear(hidden_dim // 2, num_classes)

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x

input_dim = X.shape[1]
hidden_dim = 128  # layer
num_classes = len(df['gene_encoded'].unique())
model = GeneMLPModel(input_dim, hidden_dim, num_classes)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

num_epochs = 20
for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    for features, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(features)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
    print(f"Epoch {epoch+1}, Loss: {train_loss / len(train_loader):.4f}")

model.eval()
correct = 0
total = 0
with torch.no_grad():
    for features, labels in test_loader:
        outputs = model(features)
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

accuracy = 100 * correct / total
print(f"Test Accuracy: {accuracy:.2f}%")

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
import numpy as np

train_losses = []
test_accuracies = []

all_labels = []
all_predictions = []

for epoch in range(20):
    model.train()
    train_loss = 0
    for batch in train_loader:
        features = batch['features']
        labels = batch['labels']

        optimizer.zero_grad()
        outputs = model(features)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()

    train_losses.append(train_loss / len(train_loader))

    model.eval()
    correct = 0
    total = 0
    epoch_labels = []
    epoch_predictions = []
    with torch.no_grad():
        for batch in test_loader:
            features = batch['features']
            labels = batch['labels']
            outputs = model(features)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            epoch_labels.extend(labels.tolist())
            epoch_predictions.extend(predicted.tolist())

    all_labels.extend(epoch_labels)
    all_predictions.extend(epoch_predictions)
    test_accuracy = correct / total
    test_accuracies.append(test_accuracy)

    print(f"Epoch {epoch+1}, Loss: {train_loss / len(train_loader):.4f}, Test Accuracy: {test_accuracy:.4f}")

plt.figure(figsize=(10, 6))
sns.lineplot(x=range(1, len(train_losses) + 1), y=train_losses, marker='o', label='Training Loss', color='blue')
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Loss', fontsize=12)
plt.title('Training Loss Over Epochs', fontsize=14)
plt.grid(alpha=0.5)
plt.legend(fontsize=12)
plt.show()

plt.figure(figsize=(10, 6))
sns.lineplot(x=range(1, len(test_accuracies) + 1), y=test_accuracies, marker='o', label='Test Accuracy', color='green')
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Accuracy', fontsize=12)
plt.title('Test Accuracy Over Epochs', fontsize=14)
plt.grid(alpha=0.5)
plt.legend(fontsize=12)
plt.show()

conf_matrix = confusion_matrix(all_labels, all_predictions)
plt.figure(figsize=(12, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(all_labels),
            yticklabels=np.unique(all_labels))
plt.xlabel('Predicted Labels', fontsize=12)
plt.ylabel('True Labels', fontsize=12)
plt.title('Confusion Matrix', fontsize=14)
plt.show()

print("\nClassification Report:\n")
print(classification_report(all_labels, all_predictions))

X_test

from xgboost import XGBClassifier

model = XGBClassifier()
model.fit(X_train, y_train)

accuracy = model.score(X_test, y_test)
print(f"Model Accuracy: {accuracy}")